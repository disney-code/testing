from tensorflow import keras
import random
import tensorflow as tf
import numpy as np
import time
from tensorflow.keras.datasets import cifar10
from pickle import dump,load
import os
os.environ["CUDA_VISIBLE_DEVICES"]="1" 

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)



# cifar
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train = x_train/255
x_test = x_test/255

y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

x_vali=x_train[40000:50000]
x_train=x_train[0:40000]
y_vali = y_train[40000:50000]
y_train=y_train[0:40000]

model = keras.models.load_model("./saved_models/cifar10_resnet20_model.077.h5") 

seeds=load(open("5000_indices_from_DS_and_robot/indices_5000_from_robot.pkl","rb")) #seeds is a list with length 1000
# seeds= [6433,14898,38679,25603,36562,10741,15242,14564,..] this is index into x_train
#seeds = random.sample(list(range(x_train.shape[0])), 1000)
print(f"length of seeds(should be 5000): {len(seeds)}")
images = x_train[seeds]   #images shape is 38633,32,32,3
labels = y_train[seeds]   #labels shape is 38633,32,32,3


summary_of_adv_ori_img={}  #the key is the index of image in x_train, and value is how many adv examples generated by this image

# some training samples is static, i.e., grad=<0>, hard to generate. 
seeds_filter = []
gen_img = tf.Variable(images)
grads=[]
count=0
constant_var=gen_img.shape[0]
for i in range(int(np.ceil(gen_img.shape[0]/500))):
    print(f"count {count} to count {count+500} and total count is {constant_var}")
    with tf.GradientTape() as g:
        gen_img_=tf.Variable(gen_img[count:count+500])
        labels_=labels[count:count+500]
        count+=500
        loss = keras.losses.categorical_crossentropy(labels_, model(gen_img_))
        grads.append(g.gradient(loss, gen_img_))
    del g
grads = tf.concat(grads,axis=0)
#grads shape is 1000,32,32,3
fols = np.linalg.norm((grads.numpy()+1e-20).reshape(images.shape[0], -1), ord=2, axis=1)
#fols.shape is (1000,)
seeds_filter = np.where(fols > 1e-3)[0]
#seeds_filter=array([  0,   1,   2,   3,   4,   5,   6,   7..]) it is 727 long
print("line 70 success")

start_t = time.time()
lr = 0.1
total_sets = []
for idx in seeds_filter:
    index_x_train=seeds[idx]  #get the index into x_train
    delta_t = time.time() - start_t
    if delta_t > 600:
        print(f"Total time ran was(should be 600 seconds) {delta_t}")
        break
    img_list = []
    tmp_img = images[[idx]]
    orig_img = tmp_img.copy()
    orig_norm = np.linalg.norm(orig_img)
    img_list.append(tf.identity(tmp_img))
    logits = model(tmp_img)
    orig_index = np.argmax(logits[0])
    target = keras.utils.to_categorical([orig_index], 10)
    label_top5 = np.argsort(logits[0])[-5:]

    folMAX = 0 
    epoch = 0
    adv_counter=0
    while len(img_list) > 0:
        print("working on image index: ",index_x_train)
        gen_img = img_list.pop(0)   
        for _ in range(3):
            gen_img = tf.Variable(gen_img)
            with tf.GradientTape(persistent=True) as g:
                loss = keras.losses.categorical_crossentropy(target, model(gen_img))
                grads = g.gradient(loss, gen_img)
                fol = tf.norm(grads+1e-20)
                g.watch(fol)
                logits = model(gen_img)
                obj = fol - logits[0][orig_index]
                dl_di = g.gradient(obj, gen_img)
            del g
            
            gen_img = gen_img + dl_di * lr * (random.random() + 0.5)
            gen_img = tf.clip_by_value(gen_img, clip_value_min=0, clip_value_max=1)
            
            with tf.GradientTape() as t:
                t.watch(gen_img)
                loss = keras.losses.categorical_crossentropy(target, model(gen_img))
                grad = t.gradient(loss, gen_img)
                fol = np.linalg.norm(grad.numpy()) # L2 adaption

            distance = np.linalg.norm(gen_img.numpy() - orig_img) / orig_norm
            if fol > folMAX and distance < 0.5:
                folMAX = fol
                img_list.append(tf.identity(gen_img))
            
                gen_index = np.argmax(model(gen_img)[0]) 
                if gen_index != orig_index:
                    adv_counter+=1
                    total_sets.append((fol, gen_img.numpy(), labels[idx],index_x_train))
            
#             if fol<10e-18 and distance < 0.5:
#                 img_list.append(tf.identity(gen_img))
#                 if gen_index != orig_index:
#                     adv_counter+=1
#                     total_sets.append((fol, gen_img.numpy(), labels[idx],index_x_test))
                
                
    
    summary_of_adv_ori_img[index_x_train]=adv_counter


fols = np.array([item[0] for item in total_sets])
advs = np.array([item[1].reshape(32,32,3) for item in total_sets])
labels = np.array([item[2] for item in total_sets])
original_image_index = np.array([item[3] for item in total_sets])

np.savez('./RQ3_FOL_fuzz_output/10mins_fol_RQ3/FOL_Fuzz_10mins_25Nov_5000_index.npz', advs=advs, labels=labels, fols=fols, original_img_index=original_image_index)
dump(summary_of_adv_ori_img,open("RQ3_FOL_fuzz_output/10mins_fol_RQ3/summary_10mins_25Nov_5000_index.pkl","wb"))
time_take=time.time() - start_t
dump(time_take, open("RQ3_FOL_fuzz_output/10mins_fol_RQ3/time_taken_10mins_5000_index.pkl","wb"))
seeds_filter_shape=seeds_filter.shape
dump(seeds_filter_shape, open("RQ3_FOL_fuzz_output/10mins_fol_RQ3/seeds_filter_10mins.pkl","wb"))
